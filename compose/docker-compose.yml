########################################################
#
# Use
#      docker-compose up
#
# Exported ports
#   - 172.16.208.10     6379             => redis
#   - 172.16.208.11     7001/17001       => cluster redis - node 1
#   - 172.16.208.12     7002/17002       => cluster redis - node 2
#   - 172.16.208.13     7003/17003       => cluster redis - node 3
#   - 172.16.208.14                      => redis-cluster-configure
#   - 172.16.208.15     5540             => redisinsight
#   - 172.16.208.20     5432             => postgres
#   - 172.16.208.21                      => flyway_users
#   - 172.16.208.22                      => flyway_products
#   - 172.16.208.23                      => flyway_orders
#   - 172.16.208.24                      => flyway_payments
#   - 172.16.208.30     5050             => gpadmin4
#   - 172.16.208.40     9200/9600        => OpenSearch (node 1)
#   - 172.16.208.50     5601             => OpenSearch Dashboard
#   - 172.16.208.60     4566/4571        => localstack
#   - 172.16.208.62     8181             => keycloak
#   - 172.16.208.70     32181/2888/3888  => zookeeper
#   - 172.16.208.80     9091             => kafka
#   - 172.16.208.81     9092             => kafka
#   - 172.16.208.82     9093             => kafka
#   - 172.16.208.83     8081             => kafka-schema-registry
#   - 172.16.208.90     9100             => kafka-ui
#   - 172.16.208.91     8082             => kafka-rest-proxy
#   - 172.16.208.100   10001             => gateway-swaggger-ui DISABLED
#   - 172.16.208.110    9411             => zipkin DISABLED
#   - 172.16.208.120    4318,16686       => jaeger
#   - 172.16.208.130    9090             => prometheus
#   - 172.16.208.131    9187             => postgresql-exporter
#   - 172.16.208.140    3000             => grafana
#   - 172.16.208.150    14317,14318,8889 => otel-collector
#
# monitoring        10000
# user-service      6061
# products-service  6062
# orders-service    6063,9464
# limiter-kafka-mps 7070
# limiter-processor 7071
# admin-bff         5051
# gateway           8080
#
# docker-compose up

networks:
  backend:
    name: local-development
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.16.208.0/24

services:

  redis:
    image: 'redis:7.4-alpine'
    container_name: redis
    restart: unless-stopped
    profiles: ["", "local", "kind", "redis"]
    ports:
      - '6379:6379'
    networks:
      backend:
        ipv4_address: 172.16.208.10

  redis-1:
    image: "redis:7.4-alpine"
    hostname: redis-1
    profiles: ["", "local", "kind", "redis"]
    ports:
      - "7001:7001"  # Map external port for Redis commands
      - "17001:17001"  # Map external port for Redis cluster bus
    networks:
      backend:
        ipv4_address: 172.16.208.11
    command: redis-server --maxmemory 1gb --port 7001 --bind 0.0.0.0 --maxmemory-policy allkeys-lru --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly-1.aof --cluster-announce-ip 172.16.208.11 --cluster-announce-port 7001 --cluster-announce-bus-port 17001
    healthcheck:
      interval: 5s
      timeout: 10s
      retries: 3
      test: "bash -c 'redis-cli -h localhost -p 7001 ping | grep PONG && redis-cli -h localhost -p 7001 cluster info | grep cluster_state:ok'"

  redis-2:
    image: "redis:7.4-alpine"
    hostname: redis-2
    profiles: ["", "local", "kind", "redis"]
    ports:
      - "7002:7002"  # Map external port for Redis commands
      - "17002:17002"  # Map external port for Redis cluster bus
    networks:
      backend:
        ipv4_address: 172.16.208.12
    command: redis-server --maxmemory 1gb --port 7002 --bind 0.0.0.0 --maxmemory-policy allkeys-lru --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly-2.aof --cluster-announce-ip 172.16.208.12 --cluster-announce-port 7002 --cluster-announce-bus-port 17002
    healthcheck:
      interval: 5s
      timeout: 10s
      retries: 3
      test: "bash -c 'redis-cli -h localhost -p 7002 ping | grep PONG && redis-cli -h localhost -p 7002 cluster info | grep cluster_state:ok'"

  redis-3:
    image: "redis:7.4-alpine"
    hostname: redis-3
    profiles: ["", "local", "kind", "redis"]
    ports:
      - "7003:7003"  # Map external port for Redis commands
      - "17003:17003"  # Map external port for Redis cluster bus
    networks:
      backend:
        ipv4_address: 172.16.208.13
    command: redis-server --maxmemory 1gb --port 7003 --bind 0.0.0.0 --maxmemory-policy allkeys-lru --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly-3.aof --cluster-announce-ip 172.16.208.13 --cluster-announce-port 7003 --cluster-announce-bus-port 17003
    healthcheck:
      interval: 5s
      timeout: 10s
      retries: 3
      test: "bash -c 'redis-cli -h localhost -p 7003 ping | grep PONG && redis-cli -h localhost -p 7003 cluster info | grep cluster_state:ok'"

  redis-cluster-configure:
    image: "redis:7.4-alpine"
    profiles: ["", "local", "kind", "redis"]
    networks:
      backend:
        ipv4_address: 172.16.208.14
    depends_on:
      - redis-1
      - redis-2
      - redis-3
    volumes:
      - ./:/mnt
    command: redis-cli --cluster create 172.16.208.11:7001 172.16.208.12:7002 172.16.208.13:7003 --cluster-replicas 0 --cluster-yes

  redisinsight:
    image: "redis/redisinsight:2.56"
    container_name: redisinsight
    restart: unless-stopped
    profiles: ["", "local", "kind", "redis"]
    ports:
      - "5540:5540"
    networks:
      backend:
        ipv4_address: 172.16.208.15

#  redis-commander:
#    image: rediscommander/redis-commander:latest
#    restart: "no"
#    environment:
#    - REDIS_HOSTS=local:redis:6379
#    ports:
#    - '7000:8081'
#    networks:
#      backend:
#        ipv4_address: 172.16.208.15

  postgres:
    image: 'postgres:14.4'
    container_name: postgres
    restart: unless-stopped
    profiles: ["", "local", "kind", "postgres"]
    volumes:
      - './.data/postgres_data:/var/lib/postgresql/data'
      - './docker-compose-init-postgres.sh:/docker-entrypoint-initdb.d/init-db.sh'
      - ./:/mnt
    environment:
      - POSTGRES_DB=ecomm
      - POSTGRES_USER=pgadmin
      - POSTGRES_PASSWORD=1234
    ports:
      - '5432:5432'
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -d ecomm -U pgadmin" ]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      backend:
        ipv4_address: 172.16.208.20

  flyway_users:
    image: flyway/flyway:8.5
    networks:
      backend:
        ipv4_address: 172.16.208.21
    profiles: ["", "local", "kind", "postgres"]
    volumes:
      - ${ECOMM_USERS_MODEL_SQL:-./../../users-service/db-scripts/src/main/resources}:/flyway/sql
      - ./:/mnt
    depends_on:
      - postgres
    entrypoint: ["/mnt/wait-for-it.sh", "postgres:5432", "-t", "300", "--", "/mnt/docker-compose-run-flyway.sh", "deployer", "1234", "ecomm_users" ] 

  flyway_products:
    image: flyway/flyway:8.5
    networks:
      backend:
        ipv4_address: 172.16.208.22
    profiles: ["", "local", "kind", "postgres"]
    volumes:
      - ${ECOMM_PRODUCTS_MODEL_SQL:-./../../products-service/db-scripts/src/main/resources}:/flyway/sql
      - ./:/mnt
    depends_on:
      - postgres
    entrypoint: ["/mnt/wait-for-it.sh", "postgres:5432", "-t", "300", "--", "/mnt/docker-compose-run-flyway.sh", "deployer", "1234", "ecomm_products" ] 

  flyway_orders:
    image: flyway/flyway:8.5
    networks:
      backend:
        ipv4_address: 172.16.208.23
    profiles: ["", "local", "kind", "postgres"]
    volumes:
      - ${ECOMM_ORDERS_MODEL_SQL:-./../../orders-service/db-scripts/src/main/resources}:/flyway/sql
      - ./:/mnt
    depends_on:
      - postgres
    entrypoint: ["/mnt/wait-for-it.sh", "postgres:5432", "-t", "300", "--", "/mnt/docker-compose-run-flyway.sh", "deployer", "1234", "ecomm_orders" ] 

  flyway_payments:
    image: flyway/flyway:8.5
    networks:
      backend:
        ipv4_address: 172.16.208.24
    profiles: ["", "local", "kind", "postgres"]
    volumes:
      - ${ECOMM_PAYMENTS_MODEL_SQL:-./../../payments-service/db-scripts}:/flyway/sql
      - ./:/mnt
    depends_on:
      - postgres
    entrypoint: ["/mnt/wait-for-it.sh", "postgres:5432", "-t", "300", "--", "/mnt/docker-compose-run-flyway.sh", "deployer", "1234", "ecomm_payments" ] 

  pgadmin4:
    image: 'thajeztah/pgadmin4'
    container_name: pgadmin4
    restart: unless-stopped
    profiles: ["", "local", "kind", "postgres"]
    ports:
      - '5050:5050'
    depends_on: 
      - 'postgres'
    networks:
      backend:
        ipv4_address: 172.16.208.30

  opensearch:
    image: opensearchproject/opensearch:2.16.0
    container_name: opensearch
    restart: unless-stopped
    profiles: ["", "local", "kind", "opensearch"]
    environment:
      - discovery.type=single-node
      - cluster.name=opensearch-cluster
      - node.name=opensearch
      - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "DISABLE_SECURITY_PLUGIN=true"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - ./.data/opensearch_data:/usr/share/opensearch/data:rw
    ports:
      - 9200:9200
      - 9600:9600 # required for Performance Analyzer
    networks:
      backend:
        ipv4_address: 172.16.208.40

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.16.0
    container_name: opensearch-dashboards
    restart: unless-stopped
    profiles: ["", "local", "kind", "opensearch"]
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true"
    networks:
      backend:
        ipv4_address: 172.16.208.50

  localstack:
    image: 'localstack/localstack:4.3.0'
    container_name: localstack
    restart: unless-stopped
    profiles: ["", "local", "kind"]
    ports:
      - "127.0.0.1:4566:4566"            # LocalStack Gateway
      - "127.0.0.1:4510-4559:4510-4559"  # external services port range
    environment:
      # LocalStack configuration: https://docs.localstack.cloud/references/configuration/
      - DEBUG=${DEBUG:-0}
      - IMAGE_NAME=localstack/localstack:4.3.0
    healthcheck:
      test: >-
        curl -sf localhost:4566/_localstack/init/ready | grep -q '"completed": true,'
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      backend:
        ipv4_address: 172.16.208.60
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

  localstack-resources:
    image: 'amazon/aws-cli'
    container_name: localstack-resources
    profiles: ["", "local", "kind"]
    networks:
      backend:
        ipv4_address: 172.16.208.61
    environment:
      - AWS_ENDPOINT_URL=http://localstack:4566
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_REGION=us-east-1
    volumes:
      - ./:/mnt
    depends_on:
      - localstack
    entrypoint: ["/mnt/wait-for-it.sh", "localstack:5432", "-t", "10", "--", "/mnt/docker-compose-localstack-resources.sh" ] 

  keycloak:
    image: quay.io/keycloak/keycloak:26.3.2
    container_name: keycloak
    restart: unless-stopped
    profiles: ["", "local", "kind"]
    networks:
      backend:
        ipv4_address: 172.16.208.62
    ports:
      - "8181:8080"
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=admin
      - KC_HEALTH_ENABLED=true
      # - KC_DB_VENDOR=postgres
      # - KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak
      # - KC_DB_USERNAME=keycloak
      # - KC_DB_PASSWORD=keycloak

  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.0
    container_name: zookeeper
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    ports:
      - '32181:32181'
      - '2888:2888'
      - '3888:3888'
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    healthcheck:
      test: echo stat | nc localhost 32181
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      backend:
        ipv4_address: 172.16.208.70
    logging:
      driver: "json-file"
      options:
        max-size: "1m"


    # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
    # An important note about accessing Kafka from clients on other machines:
    # -----------------------------------------------------------------------
    #
    # The config used here exposes port 9092 for _external_ connections to the broker
    # i.e. those from _outside_ the docker network. This could be from the host machine
    # running docker, or maybe further afield if you've got a more complicated setup.
    # If the latter is true, you will need to change the value 'localhost' in
    # KAFKA_ADVERTISED_LISTENERS to one that is resolvable to the docker host from those
    # remote clients
    #
    # For connections _internal_ to the docker network, such as from other services
    # and components, use kafka:29092.
    #
    # See https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
    # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
  kafka-broker-1:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka-broker-1
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    ports:
      - '9091:9091'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-broker-1:9091,EXTERNAL://localhost:19091
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19101
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9091
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      backend:
        ipv4_address: 172.16.208.80
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-broker-2:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka-broker-2
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    ports:
      - '9092:9092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-broker-2:9092,EXTERNAL://localhost:19092
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19102
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9092
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      backend:
        ipv4_address: 172.16.208.81
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-broker-3:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka-broker-3
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    ports:
      - '9093:9093'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-broker-3:9093,EXTERNAL://localhost:19093
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19103
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9093
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      backend:
        ipv4_address: 172.16.208.82
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-schema-registry:
    image: confluentinc/cp-schema-registry:7.7.0
    container_name: kafka-schema-registry
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    networks:
      backend:
        ipv4_address: 172.16.208.83
    ports:
      - '8081:8081'
    depends_on:
      - zookeeper
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-broker-1:9091,kafka-broker-2:9092,kafka-broker-3:9093
      SCHEMA_REGISTRY_HOST_NAME: kafka-schema-registry

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    ports:
      - "9100:8080"
    depends_on:
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
    environment:
      KAFKA_CLUSTERS_0_NAME: kafka-broker-1
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker-1:9091
      KAFKA_CLUSTERS_1_NAME: kafka-broker-2
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: kafka-broker-2:9092
      KAFKA_CLUSTERS_2_NAME: kafka-broker-3
      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: kafka-broker-3:9093
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      backend:
        ipv4_address: 172.16.208.90
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-rest-proxy:
    # https://github.com/confluentinc/kafka-rest
    # https://github.com/robcowart/docker_compose_cookbook/blob/master/confluent_kafka-rest/docker-compose.yml
    image: confluentinc/cp-kafka-rest:7.7.0
    container_name: kafka-rest-proxy
    restart: unless-stopped
    profiles: ["", "local", "kafka"]
    networks:
      backend:
        ipv4_address: 172.16.208.91
    ports:
      - "8082:8082"
    depends_on:
      - zookeeper
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
      - kafka-schema-registry
      - keycloak
    environment:
      KAFKA_REST_HOST_NAME: kafka-rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka-broker-1:9091,PLAINTEXT://kafka-broker-2:9092,PLAINTEXT://kafka-broker-3:9093'
      KAFKA_REST_LISTENERS: http://0.0.0.0:8082
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
      # Configuracion OAuth
      # KAFKA_REST_AUTHENTICATION: true
      # KAFKA_REST_AUTHENTICATION_METHOD: OAUTH
      # KAFKA_REST_AUTHENTICATION_REALM: ecomm
      # KAFKA_REST_AUTHENTICATION_TYPE: Bearer
      # KAFKA_REST_AUTHENTICATION_OAUTH_CLIENT_ID: ecomm-kafka-rest-client
      # KAFKA_REST_AUTHENTICATION_OAUTH_CLIENT_SECRET: ecomm-kafka-rest-client-secret
      # KAFKA_REST_AUTHENTICATION_OAUTH_TOKEN_ENDPOINT_URI: 'http://keycloak:8181/realms/ecomm/protocol/openid-connect/token'

#  gateway-swaggger-ui:
#    image: 'swaggerapi/swagger-ui:v5.17.14'
#    restart: "no"
#    ports:
#      - '10001:8080'
#    environment:
#      - SWAGGER_JSON_URL=http://localhost:8080/api
#    networks:
#      backend:
#        ipv4_address: 172.16.208.100
   

#  zipkin:
#    image: openzipkin/zipkin:3.4
#    container_name: zipkin
#    restart: unless-stopped
#    profiles: ["", "local"]
#    ports:
#      - 9411:9411
#    networks:
#      backend:
#        ipv4_address: 172.16.208.110
      
  jaeger:
    image: jaegertracing/all-in-one:1.60.0
    container_name: jaeger
    restart: unless-stopped
    profiles: ["", "local","collector"]
    ports:
      - "4318:4318"
      - "16686:16686"
    networks:
      backend:
        ipv4_address: 172.16.208.120
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    healthcheck:
      test: nc -vz localhost 4318
      interval: 10s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus:v2.55.0
    container_name: prometheus
    restart: unless-stopped
    profiles: ["", "local"]
    ports:
      - 9090:9090
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      backend:
        ipv4_address: 172.16.208.130
    healthcheck:
      test: nc -vz localhost 9090
      interval: 10s
      timeout: 10s
      retries: 3

  postgresql-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgresql-exporter
    restart: unless-stopped
    profiles: ["", "local"]
    ports:
      - "9187:9187"
    environment:
      DATA_SOURCE_NAME: "postgres://deployer:1234@postgres/ecomm_users?sslmode=disable"
    depends_on:
      prometheus:
        condition: service_started
      postgres:
        condition: service_healthy
    networks:
      backend:
        ipv4_address: 172.16.208.131

  grafana:
    image: grafana/grafana:11.3.0
    container_name: grafana
    restart: unless-stopped
    profiles: ["", "local"]
    ports:
      - "3000:3000"
    volumes:
      - ./grafana_datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    networks:
      backend:
        ipv4_address: 172.16.208.140
    depends_on:
      prometheus:
        condition: service_healthy
      jaeger:
        condition: service_healthy
    healthcheck:
      test: nc -vz localhost 3000
      interval: 10s
      timeout: 10s
      retries: 3

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.113.0
    container_name: otel-collector
    restart: unless-stopped
    profiles: ["", "local","collector"]
    ports:
      - "14317:14317" # OTLP gRPC receiver port
      - "14318:14318" # OTLP Http receiver port
      - "8889:8889" # Promentheus exporter metrics
    volumes:
      - ./otel-collector-config.yml:/etc/config.yml
    networks:
      backend:
        ipv4_address: 172.16.208.150
    command: ["--config=/etc/config.yml"]
    depends_on:
      jaeger:
        condition: service_healthy
    healthcheck:
      test: nc -vz localhost 4317
      interval: 10s
      timeout: 10s
      retries: 3
